{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\дом.DESKTOP-A3U9TO1\\AppData\\Local\\Temp\\ipykernel_8852\\2971520636.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=\"C:\\\\Users\\\\дом.DESKTOP-A3U9TO1\\\\DataspellProjects\\\\moretech_prokhodilimimo\\\\chromedriver\\\\chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"cad7952e0bd34d06ae925024215fe00f\", element=\"29d597f3-9e3f-4ae7-835a-bad675e9c0d6\")>\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"cad7952e0bd34d06ae925024215fe00f\", element=\"29d597f3-9e3f-4ae7-835a-bad675e9c0d6\")>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://pro.rbc.ru/interest/finance'\n",
    "links = []\n",
    "driver = webdriver.Chrome(executable_path=\"C:\\\\Users\\\\дом.DESKTOP-A3U9TO1\\\\DataspellProjects\\\\moretech_prokhodilimimo\\\\chromedriver\\\\chromedriver.exe\")\n",
    "\n",
    "try:\n",
    "    driver.get(url=url)\n",
    "    time.sleep(1)\n",
    "    print(1)\n",
    "    elem = driver.find_element(By.TAG_NAME, \"body\")\n",
    "    print(elem)\n",
    "    no_of_pagedowns = 100\n",
    "    while no_of_pagedowns:\n",
    "        elem.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(0.2)\n",
    "        no_of_pagedowns-=1\n",
    "    html = driver.page_source\n",
    "\n",
    "    print(html)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    driver.close()\n",
    "    driver.quit()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "#URL = 'https://www.rbc.ru/neweconomy/'\n",
    "URL = 'https://www.rbc.ru/'\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    r = requests.get(url).text\n",
    "    return BeautifulSoup(r, 'lxml')\n",
    "\n",
    "\n",
    "def save_json(data):\n",
    "    with open('rbk_data1.json', \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def main():\n",
    "    news_data = {}\n",
    "\n",
    "    soup = get_soup(URL)\n",
    "\n",
    "    # Получаем все ссылки на новости\n",
    "    news_links = soup.find('div', class_='main').find_all('a', {'class': ['main__big__link', 'main__feed__link']})\n",
    "\n",
    "    # Для каждой ссылки получаем информацию и записываем в news_data\n",
    "    for i in range(12):\n",
    "\n",
    "        link = news_links[i].get('href').split('?')[0]\n",
    "        name = link\n",
    "        news_data[name] = {}\n",
    "        soup = get_soup(link)\n",
    "\n",
    "        # Переходим на страницу для дальнейшенго парсинга\n",
    "        article = soup.find('div', class_='article')\n",
    "        #print(article.find('a'))\n",
    "        category = article.find('a', class_='article__header__category')\n",
    "\n",
    "        #date = article.find('span', class_='article__header__date').get('content').replace('T', ' ').split('+')[0]\n",
    "\n",
    "        title = article.find('h1', class_='article__header__title-in js-slide-title')\n",
    "        image = article.find('div', class_='article__main-image')\n",
    "\n",
    "        article_paragraphs = article.find_all('p')\n",
    "        article_text = ''\n",
    "        for paragraph in article_paragraphs:\n",
    "            article_text += paragraph.text\n",
    "\n",
    "        # Заполняем полученными данными news_data\n",
    "        news_data[name]['link'] = link\n",
    "        #news_data[name]['date'] = date\n",
    "        news_data[name]['text'] = article_text.replace('\\xa0', '').replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "        try:\n",
    "            news_data[name]['title'] = title.text.strip()\n",
    "        except AttributeError:\n",
    "            news_data[name]['title'] = 'Без заголовка'\n",
    "        try:\n",
    "            news_data[name]['category'] = category.text.replace('\\n', '')\n",
    "        except AttributeError:\n",
    "            news_data[name]['category'] = 'Без категории'\n",
    "        try:\n",
    "            news_data[name]['image'] = image.find('img').get('src')\n",
    "        except AttributeError:\n",
    "            news_data[name]['image'] = 'Без обложки'\n",
    "\n",
    "    save_json(news_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07.10.2022\n",
      "07.10.2022\n",
      "07.10.2022\n",
      "07.10.2022\n",
      "06.10.2022\n",
      "04.10.2022\n",
      "04.10.2022\n",
      "03.10.2022\n",
      "28.09.2022\n",
      "29.09.2022\n",
      "29.09.2022\n",
      "27.09.2022\n",
      "28.09.2022\n",
      "26.09.2022\n",
      "26.09.2022\n",
      "20.09.2022\n",
      "16.09.2022\n",
      "14.09.2022\n",
      "15.09.2022\n",
      "07.09.2022\n",
      "06.09.2022\n",
      "01.09.2022\n",
      "29.08.2022\n",
      "25.08.2022\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "URL = 'https://www.rbc.ru/neweconomy/'\n",
    "#URL = 'https://www.rbc.ru/'\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    r = requests.get(url).text\n",
    "    return BeautifulSoup(r, 'lxml')\n",
    "\n",
    "\n",
    "def save_json(data):\n",
    "    with open('rbk_data.json', \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    news_data = {}\n",
    "\n",
    "    soup = get_soup(URL)\n",
    "\n",
    "    # Получаем все ссылки на новости\n",
    "    #print(soup.find_all('a', {'class' : ['q-item__link', ]}))\n",
    "\n",
    "    news_links = soup.find_all('a', {'class' : ['q-item__link', ]})\n",
    "\n",
    "    # Для каждой ссылки получаем информацию и записываем в news_data\n",
    "    for i in range(len(news_links)):\n",
    "\n",
    "        link = news_links[i].get('href').split('?')[0]\n",
    "        name = link\n",
    "        news_data[name] = {}\n",
    "        soup = get_soup(link)\n",
    "        #print(soup.text)\n",
    "\n",
    "        # Переходим на страницу для дальнейшенго парсинга\n",
    "        article = soup.find('div', class_='article')\n",
    "        #print(article)\n",
    "\n",
    "        #print(article.find('a'))\n",
    "        #category = article.find('a', class_='article__header__category')\n",
    "        #date = article.find('span', class_='atricle__date-update').get('content').replace('T', ' ').split('+')[0]\n",
    "        try:\n",
    "            date = soup.find('div', class_='atricle__date-update').get_text().strip().split()[1]\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        print(date)\n",
    "        title = soup.title\n",
    "        #image = article.find('div', class_='article__main-image')\n",
    "        try:\n",
    "            article_paragraphs = article.find_all('p')\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        article_text = ''\n",
    "        for paragraph in article_paragraphs:\n",
    "            article_text += paragraph.text\n",
    "        #print(article_text)\n",
    "\n",
    "        # Заполняем полученными данными news_data\n",
    "        news_data[name]['link'] = link\n",
    "        news_data[name]['date'] = date\n",
    "        news_data[name]['text'] = article_text.replace('\\xa0', '').replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "        try:\n",
    "            news_data[name]['title'] = title.text\n",
    "        except AttributeError:\n",
    "            news_data[name]['title'] = 'Без заголовка'\n",
    "\n",
    "\n",
    "    save_json(news_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
