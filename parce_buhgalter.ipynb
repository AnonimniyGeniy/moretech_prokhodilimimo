{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "\n",
    "#URL = 'https://www.rbc.ru/neweconomy/'\n",
    "URL = 'https://www.buhgalteria.ru'\n",
    "\n",
    "\n",
    "def save_json(data):\n",
    "    with open('buh_data.json', \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def get_soup(url:str, params:dict=dict()):\n",
    "    r = requests.get(url, params)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    if r.status_code != 200:\n",
    "        raise Exception()\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "def get_page_links(rubric_url):\n",
    "    soup = get_soup(rubric_url)\n",
    "    return [{\"rubric\": rubric_url.split(\"/\")[-2], \"href\": j[\"href\"], \"title\": j.text}\n",
    "            for j in [i.find(\"a\") for i in soup.find_all(\"h3\")]\n",
    "            if \"article\" in j[\"href\"]]\n",
    "\n",
    "def get_all_article_links():\n",
    "    main_page_url = \"https://www.buhgalteria.ru/\"\n",
    "    soup = get_soup(main_page_url)\n",
    "    categories = [i[\"href\"] for i in soup.find(\"ul\", {\"class\": \"dropnav-menu\"}).find_all(\"a\")]\n",
    "    categories = [href for href in categories if \"rubric\" in href]\n",
    "\n",
    "    all_page_links = list()\n",
    "    all_page_hrefs = set()\n",
    "    for rubric in categories:\n",
    "        page_links = get_page_links(rubric_url=rubric)\n",
    "        for link in page_links:\n",
    "            link[\"href\"] = link[\"href\"].split(\"?\")[0]\n",
    "            if \"https://www.buhgalteria.ru\" not in link[\"href\"]:\n",
    "                link[\"href\"] = \"https://www.buhgalteria.ru\" + link[\"href\"]\n",
    "            if link[\"href\"] not in all_page_hrefs:\n",
    "                all_page_hrefs.add(link[\"href\"])\n",
    "                all_page_links.append(link)\n",
    "    return all_page_links\n",
    "\n",
    "all_pages = get_all_article_links()\n",
    "#print(all_pages)\n",
    "def main():\n",
    "    news_data = {}\n",
    "\n",
    "    soup = get_soup(URL)\n",
    "    news_links = []\n",
    "    for i in all_pages:\n",
    "        news_links.append(i['href'])\n",
    "    # Для каждой ссылки получаем информацию и записываем в news_data\n",
    "    for i in range(len(news_links)):\n",
    "\n",
    "        #link = news_links[i].get('href').split('?')[0]\n",
    "        link = news_links[i]\n",
    "        name = link\n",
    "        news_data[name] = {}\n",
    "        soup = get_soup(link)\n",
    "\n",
    "        # Переходим на страницу для дальнейшенго парсинга\n",
    "        article = soup.find('div', class_='article')\n",
    "        #print(article.find('a'))\n",
    "        category = article.find('a', class_='article__header__category')\n",
    "\n",
    "        #date = article.find('span', class_='article__header__date').get('content').replace('T', ' ').split('+')[0]\n",
    "\n",
    "        title = article.find('h1')\n",
    "        image = article.find('div', class_='article__main-image')\n",
    "\n",
    "        article_paragraphs = article.find_all('p')\n",
    "        article_text = ''\n",
    "        for paragraph in article_paragraphs:\n",
    "            article_text += paragraph.text\n",
    "\n",
    "        # Заполняем полученными данными news_data\n",
    "        news_data[name]['link'] = link\n",
    "        #news_data[name]['date'] = date\n",
    "        news_data[name]['text'] = article_text.replace('\\xa0', '').replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "        try:\n",
    "            news_data[name]['title'] = title.text.strip()\n",
    "        except AttributeError:\n",
    "            news_data[name]['title'] = 'Без заголовка'\n",
    "        try:\n",
    "            news_data[name]['category'] = category.text.replace('\\n', '')\n",
    "        except AttributeError:\n",
    "            news_data[name]['category'] = 'B'\n",
    "\n",
    "    save_json(news_data)\n",
    "    frame = pd.DataFrame(news_data).transpose()\n",
    "\n",
    "    frame.to_csv(\"buh_data.csv\", )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_json(news_data)\n",
    "frame = pd.DataFrame(news_data).transpose()\n",
    "\n",
    "frame.to_csv(\"buh_data.csv\", )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
